## Alpaca

[tloen/alpaca-lora: Instruct-tune LLaMA on consumer hardware (github.com)](https://github.com/tloen/alpaca-lora)

https://github.com/LC1332/Chinese-alpaca-lora

## LLaMA

* [0é—¨æ§›å…‹éš†ChatGPTæ–¹æ¡ˆå†å‡çº§ï¼Œå¼€æºæ¨¡å‹å®Œæ•´å¤ç°ï¼Œåœ¨çº¿ä½“éªŒæ— éœ€æ³¨å†Œ (qq.com)](https://mp.weixin.qq.com/s/V5pCvYvkPXwiMw-FNIErXw)
* [hpcaitech/ColossalAI: Making large AI models cheaper, faster and more accessible (github.com)](https://github.com/hpcaitech/ColossalAI)
* [ymcui/Chinese-LLaMA-Alpaca: ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°éƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
* [ä½¿ç”¨ Docker å’Œ Alpaca LoRA å¯¹ LLaMA 65B å¤§æ¨¡å‹è¿›è¡Œ Fine-Tune - è‹æ´‹åšå®¢ (soulteary.com)](https://soulteary.com/2023/03/25/model-finetuning-on-llama-65b-large-model-using-docker-and-alpaca-lora.html#å¯¹-llama-7b-å¤§æ¨¡å‹è¿›è¡Œ-fine-tune)
* [tloen/alpaca-lora: Instruct-tune LLaMA on consumer hardware (github.com)](https://github.com/tloen/alpaca-lora)
* [ã€å¼€æºGPTã€‘ä¸‰ä½åäººå°å“¥å¼€æºä¸­æ–‡è¯­è¨€æ¨¡å‹â€œéª†é©¼â€ï¼Œå•å¡å³å¯å®Œæˆè®­ç»ƒéƒ¨ç½²ï¼ŒèŠ±è´¹å‡ ç™¾è®­ç»ƒè‡ªå·±çš„ä¸­æ–‡èŠå¤©æ¨¡å‹ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/615968438)
* [ydli-ai/Chinese-ChatLLaMA: ä¸­æ–‡LLaMAåŸºç¡€æ¨¡å‹ï¼›ä¸­æ–‡ChatLLaMAå¯¹è¯æ¨¡å‹ï¼›é¢„è®­ç»ƒ/æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† (github.com)](https://github.com/ydli-ai/Chinese-ChatLLaMA)
* [LianjiaTech/BELLE: BELLE: BE Large Language model Engineï¼ˆå¼€æºä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼‰ (github.com)](https://github.com/LianjiaTech/BELLE)

* [3090å•å¡5å°æ—¶ï¼Œæ¯ä¸ªäººéƒ½èƒ½è®­ç»ƒä¸“å±ChatGPTï¼Œæ¸¯ç§‘å¤§å¼€æºLMFlow](https://zhuanlan.zhihu.com/p/620221835)
* [hpcaitech/ColossalAI: Making large AI models cheaper, faster and more accessible (github.com)](https://github.com/hpcaitech/ColossalAI)

## chatGLM

* [visual-openllm/visual-openllm: something like visual-chatgpt, æ–‡å¿ƒä¸€è¨€çš„å¼€æºç‰ˆ (github.com)](https://github.com/visual-openllm/visual-openllm)
* [glm-finetuneåº”ç”¨-lich99/ChatGLM-finetune-LoRA: Code for fintune ChatGLM-6b using low-rank adaptation (LoRA) (github.com)](https://github.com/lich99/ChatGLM-finetune-LoRA)
* [mymusise/ChatGLM-Tuning: ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ, åŸºäºChatGLM-6B + LoRA (github.com)](https://github.com/mymusise/ChatGLM-Tuning)
* [!!!27182812/ChatGLM-chinese-insturct: æ¢ç´¢ä¸­æ–‡instructæ•°æ®åœ¨ChatGLM-6Bä¸Šå¾®è°ƒè¡¨ç° (github.com)](https://github.com/27182812/ChatGLM-chinese-insturct)
* [liangwq/Chatglm_lora_multi-gpu: chatglmå¤šgpuç”¨deepspeedå’Œ (github.com)](https://github.com/liangwq/Chatglm_lora_multi-gpu)
* [zero_nlp/simple_thu_chatglm6b at main Â· yuanzhoulvpi2017/zero_nlp (github.com)](https://github.com/yuanzhoulvpi2017/zero_nlp/tree/main/simple_thu_chatglm6b)

## ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨

* [gpt4IE](https://github.com/cocacola-lab/GPT4IE)
* [chatIE]([cocacola-lab/ChatIE: official repository for ChatIE paper and a tool of IE using ChatGPT. Note: we set a default openai key in the tool, you can tell us if reach limit. The response speed depends on the official openai chatgpt api. ( sometimes, the official is too crowded and the speed/model will be slow/overloaded.) (github.com)](https://github.com/cocacola-lab/ChatIE))
* [åŸºäºChatGPTçš„æƒ…æ„Ÿåˆ†æï¼Œåˆ†åˆ«å¯¹æ¯”äº†ç™¾åº¦å’Œchatgptçš„æ•ˆæœ]([taishan1994/ChatSA: åŸºäºChatGPTçš„æƒ…æ„Ÿåˆ†æ (github.com)](https://github.com/taishan1994/ChatSA))
* [visual-openllm/visual-openllm: something like visual-chatgpt, æ–‡å¿ƒä¸€è¨€çš„å¼€æºç‰ˆ (github.com)](https://github.com/visual-openllm/visual-openllm)
* [WangRongsheng/ChatGenTitle: ğŸŒŸ ChatGenTitleï¼šä½¿ç”¨ç™¾ä¸‡arXivè®ºæ–‡ä¿¡æ¯åœ¨LLaMAæ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒçš„è®ºæ–‡é¢˜ç›®ç”Ÿæˆæ¨¡å‹ (github.com)](https://github.com/WangRongsheng/ChatGenTitle)
* [Moonvy/OpenPromptStudio: ğŸ¥£ AIGC æç¤ºè¯å¯è§†åŒ–ç¼–è¾‘å™¨ (github.com)](https://github.com/Moonvy/OpenPromptStudio)

## å…¶ä»–èµ„æº

* [nichtdax/awesome-totally-open-chatgpt: A list of totally open alternatives to ChatGPT (github.com)](https://github.com/nichtdax/awesome-totally-open-chatgpt)

* [ColossalChatï¼šå®Œæ•´RLHFå¹³æ›¿ChatGPTçš„å¼€æºæ–¹æ¡ˆ](https://zhuanlan.zhihu.com/p/618048558)